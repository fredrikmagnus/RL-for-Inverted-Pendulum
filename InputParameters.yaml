run:
  type: test # train or test
  num_episodes: 1000 # Number of episodes to run

animate:
  x_lim: 2 # x limit for the animation (in meters)
  y_lim: 2 # y limit for the animation (in meters)
  save_path: null # File path to save animation to. null for no save

  from_log:
    enable: False # Enable animation from file, else run simulation and animate
    episode: -1 # Log index of episode to animate. -1 for last episode
    file_path: "logs/DDPG.csv" # File path to load log from
  
pendulum:
  deterministic_initial_state: False # Use a deterministic initial state or add gaussian noise
  initial_state: down # Initial state of the pendulum (up, down or random)
  time_step: 0.05 # Time step for the simulation
  mass_bob: 1.0 # Mass of the bob
  mass_base: 1.0 # Mass of the base
  length: 1 # Length of the pendulum
  damping_coefficient: 0.1 # Damping coefficient for the pendulum
  gravity: 9.81 # Acceleration due to gravity

  termination:
    angle_limit: null # Angle limit for the pendulum (in degrees). Set to 'null' for no limit.
    x_limit: 2 # x limit for the pendulum (in meters). Set to None for no limit.
    time_limit: 10 # Time limit for the episode (in seconds). Must be greater than 0.
    termination_penalty: -5 # Penalty for terminating the episode early.

  logging: 
    enable: True # Enable logging
    file_path: "logs/DDPG.csv" # File path to save log to

model:
  type: DDPG # DQN, REINFORCE or DDPG
  memory_size: 2560 # Size of the replay memory buffer
  batch_size: 64 # Mini-batch size for training
  discount_factor: 0.99 # Discount factor for future rewards
  force_magnitude: 25 # Magnitude of the force applied to the pendulum (max force for DDPG)

  IO_parameters: # Input and output parameters for the model
    init_from_weights: 
      enable: False # Initialize model from weights
      file_path: "weights/DDPG.keras" # File path to load weights from
    save_weights:
      enable: True # Save weights to file
      file_path: "weights/DDPG.keras" # File path to save weights to
      save_frequency: 1 # Save weights every n episodes

  DQN:
    hidden_layer_sizes: [64, 64] # Number of neurons in each layer
    learning_rate: 1e-3 # Learning rate for the optimizer
    num_actions: 2 # Number of actions in the action space (2 for left and right, 3 for left, right and no action)
    polyak: 0.93 # Polyak averaging parameter for target network updates
    epsilon: # Epsilon-greedy parameters
      epsilon_init: 0.1 # Initial epsilon value (for epsilon-greedy policy)
      epsilon_min: 0.1 # Minimum epsilon value
      epsilon_decay: 0.999 # Epsilon decay rate (per mini-batch)
    
  REINFORCE:
    hidden_layer_sizes: [64, 64] # Number of neurons in each layer
    learning_rate: 0.001 # Learning rate for the optimizer
    num_actions: 2 # Number of actions in the action space (2 for left and right, 3 for left, right and no action)

  DDPG:
    actor:
      hidden_layer_sizes: [256, 256] # Number of neurons in each layer
      learning_rate: 0.001 # Learning rate for the optimizer
      polyak: 0.90 # Polyak averaging parameter for target network updates
      
      ornstein_uhlenbeck_noise:
        enable: True # Enable addition of noise to action for exploration
        theta: 1 # Theta parameter for the Ornstein-Uhlenbeck noise process
        sigma: 0.2 # Sigma parameter for the Ornstein-Uhlenbeck noise process
        noise_scale_initial: 1 # Initial noise scale
        noise_decay: 0.9995 # Noise decay rate (per episode)

    critic:
      state_input_layer_sizes: [16, 32] # Number of neurons in each of the layers processing the state input
      action_input_layer_sizes: [32] # Number of neurons in each of the layers processing the action input
      combined_layer_sizes: [256, 256] # Number of neurons in each layer for combined state and action input
      learning_rate: 0.002 # Learning rate for the optimizer
      polyak: 0.90 # Polyak averaging parameter for target network updates



  


  

    

  


  