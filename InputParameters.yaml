run:
  type: train # train or test
  num_episodes: 512 # Number of episodes to run
  # episode_time: 25 # Maximum episode time in seconds
  # dt: 0.025 # Time step in seconds
  # batch_size: 16 # Mini-batch size for training

pendulum:
  deterministic_initial_state: False # Use a deterministic initial state or add gaussian noise
  initial_state: down # Initial state of the pendulum (up, down)
  time_step: 0.05 # Time step for the simulation
  mass_bob: 1.0 # Mass of the bob
  mass_base: 1.0 # Mass of the base
  length: 0.5 # Length of the pendulum
  damping_coefficient: 0.1 # Damping coefficient for the pendulum
  gravity: 9.81 # Acceleration due to gravity

  termination:
    angle_limit: null # Angle limit for the pendulum (in degrees). Set to 'null' for no limit.
    x_limit: 3 # x limit for the pendulum (in meters). Set to None for no limit.
    time_limit: 7 # Time limit for the episode (in seconds). Must be greater than 0.
    termination_penalty: -5 # Penalty for terminating the episode early.

  logging: 
    enable: True # Enable logging
    overwrite: False # Overwrite existing log file or append to it
    file_path: "logs/DDPG_25F_try3.csv" # File path to save log to

model:
  type: DDPG # DQN, REINFORCE or DDPG
  memory_size: 512 # Size of the replay memory buffer
  batch_size: 16 # Mini-batch size for training
  discount_factor: 0.99 # Discount factor for future rewards
  force_magnitude: 35 # Magnitude of the force applied to the pendulum (max force for DDPG)


  IO_parameters: # Input and output parameters for the model
    init_from_weights: 
      enable: True # Initialize model from weights
      file_path: "weights/DDPG_25F.keras" # File path to load weights from
    save_weights:
      enable: True # Save weights to file
      file_path: "weights/DDPG_25F.keras" # File path to save weights to
      save_frequency: 1 # Save weights every n episodes

  DQN:
    hidden_layer_sizes: [16, 16] # Number of neurons in each layer
    learning_rate: 1e-3 # Learning rate for the optimizer
    epsilon: # Epsilon-greedy parameters
      epsilon_init: 1 # Initial epsilon value (for epsilon-greedy policy)
      epsilon_min: 0.1 # Minimum epsilon value
      epsilon_decay: 0.999 # Epsilon decay rate (per mini-batch)
    
  REINFORCE:
    hidden_layer_sizes: [16, 16] # Number of neurons in each layer
    learning_rate: 1e-3 # Learning rate for the optimizer

  DDPG:
    actor:
      hidden_layer_sizes: [256, 256] # Number of neurons in each layer
      learning_rate: 0.001 # Learning rate for the optimizer
      polyak: 0.995 # Polyak averaging parameter for target network updates
      
      ornstein_uhlenbeck_noise:
        enable: True # Enable addition of noise to action for exploration
        theta: 1 # Theta parameter for the Ornstein-Uhlenbeck noise process
        sigma: 0.2 # Sigma parameter for the Ornstein-Uhlenbeck noise process
        sigma_decay: 0.999 # Sigma decay rate (per episode)

    critic:
      state_input_layer_sizes: [16, 32] # Number of neurons in each of the layers processing the state input
      action_input_layer_sizes: [32] # Number of neurons in each of the layers processing the action input
      combined_layer_sizes: [256, 256] # Number of neurons in each layer for combined state and action input
      learning_rate: 0.002 # Learning rate for the optimizer
      polyak: 0.995 # Polyak averaging parameter for target network updates


  

    

  


  